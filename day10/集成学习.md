# 集成学习

## 剪枝分类器
剪枝分类器不断生长、积累，形成多层级的模型，该模型就成为决策树分类器。对决策树分类器进行bagging学习的时候，通过随机选择输入变量中的某个纬度进行学习，可以大幅提高分类器的性能，这种手法也称为随机森林学习。

## Bagging学习法

> Bagging单词，是从BootstrapAggregation创造而来的单词。
> 在Bagging学习中，首先经由自助法生成虚拟的训练样本，并对这些样本进行学习，然后，重复这一过程，对得到的多个分类器的输出求平均值。
> 总结而言，Bagging学习是对多个弱分类器进行独立学习的方法。

1. 对j=1，2，...，b重复进行如下计算。
   (a) 从n个训练样本中$ \lbrace (x_i,y_i) \rbrace^n_{i=1} ​$随机选取$ n ​$个，允许重复，生成若干个与原始的训练样本集有些许差异的新样本集。
   (b) 使得上述得到的样本集求得弱学习器$ \varphi_j ​$。
2. 对所有的弱分类器$ \lbrace \varphi_j \rbrace^b_{i=1} $，求平均值，得到一个强分类器$ f $。

**从偏差-方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受扰动的学习器上效用更加明显。**

### 随机森林

随机森林在以决策树为基学习器构建`Bagging`集成模型的基础上，进一步在决策树的训练过程中引入了随机属性选择。

- 传统决策树在选择划分属性时，是在当前结点的属性集合（假定有  个属性）中选择一个最优属性。
- 随机森林中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含  个属性的子集，然后再从这个子集中选择一个最优属性用于划分。
  - 如果$k=n$，则基决策树的构建与传统决策树相同。
  - 如果$k=1$，则则随机选择一个属性用于划分。
  - 通常建议$k={log_2}n$。

随机森林的优点：

- 训练效率较高。因为随机森林使用的决策树只需要考虑所有属性的一个子集。
- 随机森林简单、容易实现、计算开销小。
- 随机森林在很多现实任务中展现出强大的性能，被称作 “代表集成学习技术水平的方法”。

随着树的数量的增加，随机森林可以有效缓解过拟合。因为随着树的数量增加，模型的方差会显著降低。但是树的数量增加并不会纠正偏差，因此随机森林还是会有过拟合。

## Boosting学习法

### Adaboost

>Adaboost是应为Adaptive Boosting的缩写，是自适应增强的意思。 

1. 把训练样本$ \lbrace (x_i,y_i) \rbrace^n_{i=1} ​$对应的各个权重$ \lbrace \omega_i \rbrace^n_{i=1} ​$设置为均等，即$ 1/n​$，并把强分类器$f​$的初始值设为零。

   $$\omega_1,\omega_2,\cdots,\omega_n \leftarrow 1/n,  f \leftarrow 0$$

2. 对$j = 1,2,\cdots,b​$重复进行如下计算。

   a) 对于现在的样本的权重，对加权的误分辨率(0/1损失的权重之和)$R(\varphi)$为最小的弱分类器$ \varphi_j $进行学习。

   $$ \varphi_j = \underset{\varphi}{argmim} R(\varphi)，    R(\varphi)=\sum_{i=1}^{n}\frac{w_{i}}{2}\left ( 1-\varphi\left ( x_{i} \right ) y_{i} \right )​$$

   b) 通过下式定义弱分类器$ \varphi_j $的权重$ \theta_j $。**(核心步骤之一，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值)**

   $$\theta_j=\frac{1}{2}log\frac{1-R(\varphi_j)}{R(\varphi_j)}​$$

   c) 通过下式更新强分类器$f​$。**(核心步骤之二，加大分类误差率较小的弱分类器的权值，使得它在表决中起较大作用；减小分类误差率较大的弱分类器的权值，使得它在表决中起较小的作用。)**

   $$ f \leftarrow f +\theta_j \varphi_j​$$

   d) 通过下式更新样本的权重$ \lbrace \omega_i \rbrace^n_{i=1} ​$

   $$w_i \leftarrow \frac{exp(-f(x_i)y_i)}{\sum_{{i}'=1}^{n}-f(x_{i}')y_i} , \forall i=1,2,\cdots ,n​$$

#### 注意

> 为防止过拟合，AdaBoost通常在更新强分类器时，引入正则化项，该正则化项称为步长或者学习率，定义为$v$。考虑正则化项之后，模型的更新方式为：$f \leftarrow f +v\theta_j \varphi_j$
>
> 当AdaBoost的基础分类器比较复杂时，AdaBoost很容易陷入过拟合。但是当AdaBoost 的基础分类器比较简单时，AdaBoost 反而难以陷入过拟合。这也是为什么AdaBoost 的基础分类器经常选择使用树桩的原因。

**从偏差-方差分解的角度分析，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。**

## 集成策略

1. 学习器组合可以带来好处：

   - 由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能。

     此时如果使用单学习器可能因为造成误选而导致泛化性能不佳，通过学习器组合之后会减小这一风险。

   - 学习算法往往会陷入局部极小。有的局部极小点所对应的泛化性能可能很差，而通过学习器组合之后可降低陷入糟糕局部极小的风险。

   - 某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时使用单学习器肯定无效。

     通过学习器组合之后，由于相应的假设空间有所扩大，有可能学得更好的近似。

2. 假定集成包含$M$个基学习器$h_1,h_2,\cdots,h_M$。一共有三种集成策略：

   - 平均法。
   - 投票法。
   - 学习法。

### 平均法

平均法通常用于回归任务中。

- 简单平均法：$$
- 加权平均法：

现实任务中训练样本通常不充分或者存在噪声，这就使得学得的权重不完全可靠。尤其是对于规模比较大的集成学习，要学习的权重比较多，很容易出现过拟合。

因此实验和应用均显示出，加权平均法不一定优于简单平均法。

通常如果个体学习器性能相差较大时，适合使用加权平均法；个体学习器性能相差较近时，适合使用简单平均法。



## 集成学习常见面试题

1. 什么是集成学习算法？

   集成学习通过构建并结合多个学习器来完成学习任务，集成学习的一般结构：先产生一组“个体学习器”，再用某种策略将它们结合起来。

2. 集成学习主要有哪几种框架？

   个体学习器间存在强依赖关系，必须串行生成的序列化方法。代表：Boosting

   个体学习器间不存在强依赖关系，可同时生成的并行化方法。

3. 简单介绍一下bagging，常用bagging算法有哪些？

   Bagging/随即森林

4. 简单介绍一下boosting，常用boosting算法有哪些？

   Boosting和Adaboost

5. boosting思想的数学表达式是什么？

6. 简单介绍一下stacking，常用stacking算法有哪些？

7. 你意识到你的模型受到低偏差和高方差问题的困扰，应该使用哪种算法来解决问题呢？为什么？

8. 简述一下随机森林算法的原理

9.随机森林的随机性体现在哪里？

10.随机森林为什么不容易过拟合？

11.你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？

12.如何使用随机森林去弥补特征向量中的缺失值

13.如何使用随机森林对特征重要性进行评估？

14.随机森林算法训练时主要需要调整哪些参数？

15.随机森林为什么不能用全样本去训练m棵决策树？

16.随机森林算法有哪些优缺点

17.简述一下Adaboost原理

18.AdaBoost的优点和缺点

19.为什么Adaboost对噪声敏感？

20.Adaboost和随机森林算法的异同点

 

集成学习面试题二

1.简述GBDT原理

2.GBDT常用损失函数有哪些？

3.GBDT中为什么要用负梯度来代替残差计算？

4.GBDT如何用于分类?

5.GBDT中的决策树是分类树还是回归树？

6.如何使用GBDT构建特征？

7.为什么GBDT不适合使用高维稀疏特征?

8.GBDT通过什么方式减少误差？

9. GBDT如何进行正则化？ 

10.GBDT里的G代表什么，体现在哪里？

11.GBDT需要调试的参数有哪些？

12.GBDT算法的优缺点有哪些？

13.Xgboost/GBDT在调参时为什么树的深度很少就能达到很高的精度，而随机森林需要的深度相对较高？

14.为什么Xgboost要用泰勒展开，优势在哪里？

15.Xgboost如何寻找最优特征？

16.Xgboost采样是有放回还是无放回的呢？

17.XGBoost训练通常调整的参数有哪些？

18.XGBoost中的树是如何剪枝？

19.XGBoost如何解决缺失值问题？

20.XGBoost和GBDT的区别
--------------------- 