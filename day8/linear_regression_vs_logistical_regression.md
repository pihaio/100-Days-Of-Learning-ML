# 重要概念 #
## 回归分析 ##
	回归分析是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。
	回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。通常用于预测分析、时间序列模型以及发现变量之间的因果关系。
## 线性回归 ##
	线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差服从均值为0的正态分布。
	在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。
	线性回归使用最佳的拟合直线（也成回归线）在因变量Y与一个或多个自变量X之间建立一种关系。
	# 要点 #
		1. 自变量与因变量之间必须有线性关系
		2. 多元回归存在多重共线性，自相关性和异方差性。
		3. 线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。
		4. 多重共线性会增加系数估计值的方差，使得在轻微变化下，估计非常敏感。结果就是系数估计值不稳定。
		5. 在多个自变量的情况下，可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。
	
### 线性回归模型 ###
![](https://i.imgur.com/E4InZoe.png)

	评估θ的优劣，需要对做出的h函数进行评价，而这个评价函数称为损失函数J函数
![](https://i.imgur.com/xPftdz3.png)
	对于线性回归，均方误差损失是一个凸函数，具有全局最小值。
### 优化方法 ###
	

- 最小二乘法
	

- 梯度下降法




## 逻辑回归 ##
	一种广义的线性回归分析模型，当因变量只能在{0,1}中取值时，线性回归模型不再适合，因为极端数据的存在会使阀值的选择变得困难。我们可以使用逻辑回归对数据进行建模。
	逻辑回归模型是一个非线性模型，Sigmoid函数，又称为逻辑回归函数。
	# 要点 #
		1. 广泛用于分类问题
		2. 逻辑回归不要求自变量与因变量是线性关系。
		3. 为了避免过拟合和欠拟合，使用逐步筛选方法来包括所有重要的变量，估计逻辑回归。
		4. 需要大的样本量，在样本数量较少的情况下，极大似然估计的效果比普通最小二乘法差。
		5. 自变量不应该相互关联的，即不具有多重共线性。
		6. 如果因变量的值是定序变量，则称它为序逻辑回归。
		7. 如果因变量是多类的话，则称它为多元逻辑回归。
		
## 多项式回归 ##
	对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。如下方程所示：
	y=a+b*x^2
	在这种回归技术中，最佳拟合线不是直线。而是一个用于拟合数据点的曲线。
	# 重点 #
		1. 然会有一个诱导可以拟合一个高次多项式并得到较低的错误，但这可能会导致过拟合。你需要经常画出关系图来查看拟合情况，并且专注于保证拟合合理，既没有过拟合又没有欠拟合。下面是一个图例，可以帮助理解：
![](https://i.imgur.com/ijNFTGz.png)


# 易混淆概念 #
样本方差:
![](https://i.imgur.com/dun1ZxJ.gif)
总体方差:
![](https://i.imgur.com/cmALHxs.png)
标准差 = 均方差:
![](https://i.imgur.com/BZOVH9n.png)
均方误差(MSE):
![](https://i.imgur.com/3H4yycZ.png)
均方根值(RMS):
![](https://i.imgur.com/wb8Vf1W.png)
均方根误差：
![](https://i.imgur.com/7uwmUkf.png)